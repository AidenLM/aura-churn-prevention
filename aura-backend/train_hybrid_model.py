"""
AURA Churn Prediction - HYBRID MODEL (Maven + Iranian Churn)
==============================================================

Bu script iki dataset'i birleÅŸtirir:
1. Maven Analytics Telecom Churn (7,043 mÃ¼ÅŸteri, 37 Ã¶zellik)
2. Iranian Churn Dataset (3,150 mÃ¼ÅŸteri, 13 Ã¶zellik)

Toplam: 10,193 mÃ¼ÅŸteri ile en gÃ¼Ã§lÃ¼ modeli eÄŸitir!

Strateji:
- Maven'dan: Churn_Reason, Lokasyon, DetaylÄ± servisler
- Iranian'dan: GerÃ§ek complaint_count, call_failures, usage patterns
- Eksik deÄŸerler: AkÄ±llÄ± imputation ile doldurulur
"""

import pandas as pd
import numpy as np
import pickle
import json
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
import xgboost as xgb
import shap
import warnings
warnings.filterwarnings('ignore')

print("=" * 80)
print("ğŸ¯ AURA HYBRID MODEL - MAVEN + IRANIAN CHURN")
print("=" * 80)
print("\nğŸš€ Ä°ki dataset'i birleÅŸtirip en gÃ¼Ã§lÃ¼ modeli oluÅŸturuyoruz!")
print("   Maven Analytics: 7,043 mÃ¼ÅŸteri")
print("   Iranian Churn: 3,150 mÃ¼ÅŸteri")
print("   TOPLAM: 10,193 mÃ¼ÅŸteri\n")

# ============================================================================
# 1. DATASET'LERÄ° YÃœKLE
# ============================================================================

print("=" * 80)
print("ğŸ“‚ DATASET'LER YÃœKLENÄ°YOR")
print("=" * 80)

from google.colab import files

# Maven Analytics Dataset - Otomatik Ä°ndir
print("\n1ï¸âƒ£  Maven Analytics dataset'i otomatik indiriliyor...")
print("   URL: https://maven-datasets.s3.amazonaws.com/Telecom+Churn/Telecom+Churn.xlsx")

try:
    df_maven = pd.read_excel('https://maven-datasets.s3.amazonaws.com/Telecom+Churn/Telecom+Churn.xlsx')
    print(f"   âœ… Maven otomatik indirildi: {df_maven.shape[0]} satÄ±r, {df_maven.shape[1]} sÃ¼tun")
except:
    print("   âš ï¸  Otomatik indirme baÅŸarÄ±sÄ±z, manuel yÃ¼kleme...")
    print("   Link: https://mavenanalytics.io/data-playground/telecom-customer-churn")
    uploaded_maven = files.upload()
    maven_file = list(uploaded_maven.keys())[0]
    df_maven = pd.read_csv(maven_file)
    print(f"   âœ… Maven yÃ¼klendi: {df_maven.shape[0]} satÄ±r, {df_maven.shape[1]} sÃ¼tun")

# Iranian Churn Dataset
print("\n2ï¸âƒ£  Iranian Churn dataset'ini yÃ¼kle:")
print("   Link: https://archive.ics.uci.edu/ml/datasets/Iranian+Churn+Dataset")
print("   Veya Kaggle'dan ara: 'Iranian Churn Dataset'")
uploaded_iranian = files.upload()
iranian_file = list(uploaded_iranian.keys())[0]
df_iranian = pd.read_csv(iranian_file)
print(f"   âœ… Iranian yÃ¼klendi: {df_iranian.shape[0]} satÄ±r, {df_iranian.shape[1]} sÃ¼tun")

print(f"\nğŸ“Š Toplam mÃ¼ÅŸteri: {df_maven.shape[0] + df_iranian.shape[0]}")

# ============================================================================
# 2. MAVEN ANALYTICS HAZIRLIK
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ”§ MAVEN ANALYTICS HAZIRLIK")
print("=" * 80)

# Churn sÃ¼tununu oluÅŸtur
if 'Customer_Status' in df_maven.columns:
    df_maven['Churn'] = (df_maven['Customer_Status'] == 'Churned').astype(int)
elif 'Churn' in df_maven.columns:
    if df_maven['Churn'].dtype == 'object':
        df_maven['Churn'] = (df_maven['Churn'] == 'Yes').astype(int)

print(f"âœ… Maven Churn rate: {df_maven['Churn'].mean()*100:.2f}%")

# Maven'dan ortak Ã¶zellikleri seÃ§ ve yeniden adlandÄ±r
maven_mapping = {
    'Tenure_in_Months': 'tenure_months',
    'Monthly_Charge': 'monthly_charge',
    'Age': 'age',
    'Gender': 'gender',
    'Number_of_Referrals': 'referrals',
    'Avg_Monthly_GB_Download': 'data_usage_gb',
    'Churn': 'churn'
}

# Maven iÃ§in yeni Ã¶zellikler tÃ¼ret
print("\nğŸ”§ Maven'dan davranÄ±ÅŸsal Ã¶zellikler tÃ¼retiliyor...")

def derive_maven_features(df):
    df_new = df.copy()
    
    # complaint_count (Churn_Reason'dan)
    def get_complaints(row):
        reason = str(row.get('Churn_Reason', ''))
        support = str(row.get('Premium_Tech_Support', 'No'))
        if 'Attitude' in reason:
            return np.random.randint(4, 8)
        elif 'Dissatisfaction' in reason:
            return np.random.randint(2, 5)
        elif support == 'Yes':
            return np.random.randint(0, 2)
        else:
            return np.random.randint(0, 3)
    
    df_new['complaint_count'] = df_new.apply(get_complaints, axis=1)
    
    # call_failures (Internet_Type'dan)
    def get_call_failures(row):
        internet = str(row.get('Internet_Type', ''))
        if internet == 'Fiber Optic':
            return np.random.randint(5, 15)
        elif internet == 'DSL':
            return np.random.randint(2, 8)
        elif internet == 'Cable':
            return np.random.randint(1, 5)
        else:
            return np.random.randint(0, 3)
    
    df_new['call_failures'] = df_new.apply(get_call_failures, axis=1)
    
    # support_calls_count
    def get_support_calls(row):
        support = str(row.get('Premium_Tech_Support', 'No'))
        complaints = row['complaint_count']
        if support == 'Yes':
            return np.random.randint(3, 8)
        elif complaints >= 4:
            return np.random.randint(5, 10)
        else:
            return np.random.randint(0, 3)
    
    df_new['support_calls_count'] = df_new.apply(get_support_calls, axis=1)
    
    # payment_delays
    def get_payment_delays(row):
        method = str(row.get('Payment_Method', ''))
        if 'Mailed' in method:
            return np.random.randint(2, 5)
        elif 'Bank' in method:
            return np.random.randint(1, 3)
        else:
            return np.random.randint(0, 2)
    
    df_new['payment_delays'] = df_new.apply(get_payment_delays, axis=1)
    
    # sms_count
    def get_sms(row):
        phone = str(row.get('Phone_Service', 'No'))
        multiple = str(row.get('Multiple_Lines', 'No'))
        if phone == 'No':
            return 0
        elif multiple == 'Yes':
            return np.random.randint(100, 300)
        else:
            return np.random.randint(20, 150)
    
    df_new['sms_count'] = df_new.apply(get_sms, axis=1)
    
    # seconds_of_use (tenure ve usage'dan)
    df_new['seconds_of_use'] = df_new.get('Tenure_in_Months', 12) * 30 * 24 * 60 * np.random.uniform(0.1, 0.3, len(df_new))
    
    # frequency_of_use
    df_new['frequency_of_use'] = np.random.randint(10, 50, len(df_new))
    
    # customer_value (Monthly_Charge'dan)
    df_new['customer_value'] = df_new.get('Monthly_Charge', 50) * df_new.get('Tenure_in_Months', 12) / 100
    
    return df_new

df_maven = derive_maven_features(df_maven)
print("âœ… Maven Ã¶zellikleri tÃ¼retildi")

# ============================================================================
# 3. IRANIAN CHURN HAZIRLIK
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ”§ IRANIAN CHURN HAZIRLIK")
print("=" * 80)

# Iranian sÃ¼tun isimlerini kontrol et ve standartlaÅŸtÄ±r
print(f"\nğŸ“‹ Iranian sÃ¼tunlar: {list(df_iranian.columns)}")

# Churn sÃ¼tununu bul
churn_col = None
for col in df_iranian.columns:
    if 'churn' in col.lower():
        churn_col = col
        break

if churn_col:
    df_iranian['churn'] = df_iranian[churn_col].astype(int)
    print(f"âœ… Iranian Churn rate: {df_iranian['churn'].mean()*100:.2f}%")
else:
    print("âš ï¸  Churn sÃ¼tunu bulunamadÄ±, son sÃ¼tun kullanÄ±lÄ±yor")
    df_iranian['churn'] = df_iranian.iloc[:, -1].astype(int)

# Iranian'dan eksik Ã¶zellikleri tÃ¼ret
print("\nğŸ”§ Iranian'dan eksik Ã¶zellikler tÃ¼retiliyor...")

def derive_iranian_features(df):
    df_new = df.copy()
    
    # EÄŸer yoksa, temel Ã¶zellikleri tÃ¼ret
    if 'age' not in df_new.columns:
        df_new['age'] = np.random.randint(18, 70, len(df_new))
    
    if 'gender' not in df_new.columns:
        df_new['gender'] = np.random.choice(['Male', 'Female'], len(df_new))
    
    if 'referrals' not in df_new.columns:
        df_new['referrals'] = np.random.randint(0, 5, len(df_new))
    
    # data_usage_gb varsa kullan, yoksa tÃ¼ret
    if 'data_usage_gb' not in df_new.columns:
        # Seconds_of_Use veya benzeri bir sÃ¼tundan tÃ¼ret
        usage_cols = [col for col in df_new.columns if 'usage' in col.lower() or 'second' in col.lower()]
        if usage_cols:
            df_new['data_usage_gb'] = df_new[usage_cols[0]] / 1000000  # Saniyeden GB'ye yaklaÅŸÄ±k
        else:
            df_new['data_usage_gb'] = np.random.uniform(5, 50, len(df_new))
    
    return df_new

df_iranian = derive_iranian_features(df_iranian)
print("âœ… Iranian Ã¶zellikleri tÃ¼retildi")

# ============================================================================
# 4. ORTAK Ã–ZELLÄ°KLERÄ° BELÄ°RLE VE BÄ°RLEÅTÄ°R
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ”— DATASET'LER BÄ°RLEÅTÄ°RÄ°LÄ°YOR")
print("=" * 80)

# Ortak Ã¶zellik listesi (her iki dataset'te de olacak)
common_features = [
    'tenure_months',
    'monthly_charge',
    'age',
    'gender',
    'complaint_count',
    'call_failures',
    'support_calls_count',
    'payment_delays',
    'data_usage_gb',
    'sms_count',
    'seconds_of_use',
    'frequency_of_use',
    'customer_value',
    'referrals',
    'churn'
]

print(f"\nğŸ“‹ Ortak Ã¶zellikler ({len(common_features)-1} Ã¶zellik + churn):")
for feat in common_features[:-1]:
    print(f"   - {feat}")

# Maven'Ä± standart formata Ã§evir
maven_standard = pd.DataFrame()
maven_standard['tenure_months'] = df_maven.get('Tenure_in_Months', 12)
maven_standard['monthly_charge'] = df_maven.get('Monthly_Charge', 50)
maven_standard['age'] = df_maven.get('Age', 35)
maven_standard['gender'] = df_maven.get('Gender', 'Male')
maven_standard['complaint_count'] = df_maven['complaint_count']
maven_standard['call_failures'] = df_maven['call_failures']
maven_standard['support_calls_count'] = df_maven['support_calls_count']
maven_standard['payment_delays'] = df_maven['payment_delays']
maven_standard['data_usage_gb'] = df_maven.get('Avg_Monthly_GB_Download', df_maven.get('data_usage_gb', 20))
maven_standard['sms_count'] = df_maven['sms_count']
maven_standard['seconds_of_use'] = df_maven['seconds_of_use']
maven_standard['frequency_of_use'] = df_maven['frequency_of_use']
maven_standard['customer_value'] = df_maven['customer_value']
maven_standard['referrals'] = df_maven.get('Number_of_Referrals', 0)
maven_standard['churn'] = df_maven['churn']
maven_standard['source'] = 'maven'

print(f"\nâœ… Maven standardize edildi: {maven_standard.shape}")

# Iranian'Ä± standart formata Ã§evir
iranian_standard = pd.DataFrame()

# SÃ¼tun isimlerini eÅŸle (Iranian dataset'teki gerÃ§ek sÃ¼tun isimlerine gÃ¶re)
iranian_cols = df_iranian.columns.tolist()

# Dinamik eÅŸleme
def find_column(keywords, columns):
    for keyword in keywords:
        for col in columns:
            if keyword.lower() in col.lower():
                return col
    return None

iranian_standard['tenure_months'] = df_iranian.get(
    find_column(['subscription', 'tenure', 'length'], iranian_cols), 
    np.random.randint(1, 60, len(df_iranian))
)
iranian_standard['monthly_charge'] = df_iranian.get(
    find_column(['charge', 'amount', 'tariff'], iranian_cols),
    np.random.uniform(20, 100, len(df_iranian))
)
iranian_standard['age'] = df_iranian.get('age', np.random.randint(18, 70, len(df_iranian)))
iranian_standard['gender'] = df_iranian.get('gender', np.random.choice(['Male', 'Female'], len(df_iranian)))
iranian_standard['complaint_count'] = df_iranian.get(
    find_column(['complain', 'complaint'], iranian_cols),
    np.random.randint(0, 5, len(df_iranian))
)
iranian_standard['call_failures'] = df_iranian.get(
    find_column(['call', 'failure'], iranian_cols),
    np.random.randint(0, 10, len(df_iranian))
)
iranian_standard['support_calls_count'] = df_iranian.get(
    find_column(['customer', 'service', 'call'], iranian_cols),
    np.random.randint(0, 8, len(df_iranian))
)
iranian_standard['payment_delays'] = np.random.randint(0, 5, len(df_iranian))
iranian_standard['data_usage_gb'] = df_iranian.get('data_usage_gb', np.random.uniform(5, 50, len(df_iranian)))
iranian_standard['sms_count'] = df_iranian.get(
    find_column(['sms', 'frequency'], iranian_cols),
    np.random.randint(0, 200, len(df_iranian))
)
iranian_standard['seconds_of_use'] = df_iranian.get(
    find_column(['second', 'usage', 'use'], iranian_cols),
    np.random.uniform(10000, 100000, len(df_iranian))
)
iranian_standard['frequency_of_use'] = df_iranian.get(
    find_column(['frequency'], iranian_cols),
    np.random.randint(10, 50, len(df_iranian))
)
iranian_standard['customer_value'] = df_iranian.get(
    find_column(['value', 'status'], iranian_cols),
    np.random.uniform(1, 5, len(df_iranian))
)
iranian_standard['referrals'] = df_iranian.get('referrals', np.random.randint(0, 5, len(df_iranian)))
iranian_standard['churn'] = df_iranian['churn']
iranian_standard['source'] = 'iranian'

print(f"âœ… Iranian standardize edildi: {iranian_standard.shape}")

# Ä°ki dataset'i birleÅŸtir
df_combined = pd.concat([maven_standard, iranian_standard], ignore_index=True)

print(f"\nğŸ‰ Dataset'ler birleÅŸtirildi!")
print(f"   Toplam mÃ¼ÅŸteri: {len(df_combined)}")
print(f"   Maven: {len(maven_standard)} ({len(maven_standard)/len(df_combined)*100:.1f}%)")
print(f"   Iranian: {len(iranian_standard)} ({len(iranian_standard)/len(df_combined)*100:.1f}%)")
print(f"   Toplam Churn rate: {df_combined['churn'].mean()*100:.2f}%")

# ============================================================================
# 5. VERÄ° TEMÄ°ZLEME VE HAZIRLIK
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ§¹ VERÄ° TEMÄ°ZLEME")
print("=" * 80)

# Eksik deÄŸerleri kontrol et
missing = df_combined.isnull().sum()
if missing.sum() > 0:
    print(f"\nâš ï¸  Eksik deÄŸerler bulundu:")
    print(missing[missing > 0])
    print(f"\nğŸ”§ Eksik deÄŸerler median ile doldurulacak...")
    for col in df_combined.columns:
        if df_combined[col].isnull().sum() > 0:
            if df_combined[col].dtype in ['float64', 'int64']:
                df_combined[col].fillna(df_combined[col].median(), inplace=True)
            else:
                df_combined[col].fillna(df_combined[col].mode()[0], inplace=True)
    print("âœ… Eksik deÄŸerler dolduruldu")
else:
    print("âœ… Eksik deÄŸer yok!")

# Outlier'larÄ± kontrol et ve temizle
print(f"\nğŸ” Outlier kontrolÃ¼...")
numeric_cols = df_combined.select_dtypes(include=[np.number]).columns.tolist()
numeric_cols.remove('churn')

for col in numeric_cols:
    Q1 = df_combined[col].quantile(0.25)
    Q3 = df_combined[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 3 * IQR
    upper_bound = Q3 + 3 * IQR
    
    outliers = ((df_combined[col] < lower_bound) | (df_combined[col] > upper_bound)).sum()
    if outliers > 0:
        print(f"   {col}: {outliers} outlier bulundu, kÄ±rpÄ±lÄ±yor...")
        df_combined[col] = df_combined[col].clip(lower_bound, upper_bound)

print("âœ… Outlier'lar temizlendi")

# ============================================================================
# 6. Ã–ZELLÄ°K HAZIRLIK VE ENCODING
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ¯ Ã–ZELLÄ°K HAZIRLIK")
print("=" * 80)

# Feature ve target ayÄ±r
feature_cols = [col for col in common_features if col != 'churn']
X = df_combined[feature_cols].copy()
y = df_combined['churn'].copy()

print(f"\nğŸ“Š X shape: {X.shape}")
print(f"ğŸ“Š y shape: {y.shape}")

# Kategorik deÄŸiÅŸkenleri encode et
categorical_cols = X.select_dtypes(include=['object']).columns.tolist()
print(f"\nğŸ”¤ Kategorik sÃ¼tunlar: {categorical_cols}")

label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col].astype(str))
    label_encoders[col] = le

print(f"âœ… {len(categorical_cols)} kategorik sÃ¼tun encode edildi")

# ============================================================================
# 7. TRAIN/TEST SPLIT
# ============================================================================

print("\n" + "=" * 80)
print("âœ‚ï¸  TRAIN/TEST SPLIT")
print("=" * 80)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"\nğŸ“Š Train: {X_train.shape[0]} samples ({y_train.mean()*100:.2f}% churn)")
print(f"ğŸ“Š Test: {X_test.shape[0]} samples ({y_test.mean()*100:.2f}% churn)")

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("âœ… Feature scaling tamamlandÄ±")

# ============================================================================
# 8. MODEL EÄÄ°TÄ°MÄ°
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ¤– XGBOOST MODEL EÄÄ°TÄ°MÄ°")
print("=" * 80)

params = {
    'objective': 'binary:logistic',
    'max_depth': 6,
    'learning_rate': 0.1,
    'n_estimators': 200,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'random_state': 42,
    'eval_metric': 'logloss'
}

print(f"\nğŸš€ Model eÄŸitimi baÅŸlÄ±yor...")
model = xgb.XGBClassifier(**params)
model.fit(
    X_train_scaled, y_train,
    eval_set=[(X_test_scaled, y_test)],
    verbose=False
)

print("âœ… Model eÄŸitimi tamamlandÄ±!")

# ============================================================================
# 9. MODEL DEÄERLENDÄ°RME
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ“Š MODEL PERFORMANSI")
print("=" * 80)

y_pred = model.predict(X_test_scaled)
y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred_proba)

print(f"\nğŸ¯ Hybrid Model PerformansÄ±:")
print(f"   Accuracy:  {accuracy*100:.2f}%")
print(f"   Precision: {precision*100:.2f}%")
print(f"   Recall:    {recall*100:.2f}%")
print(f"   F1 Score:  {f1*100:.2f}%")
print(f"   ROC AUC:   {roc_auc*100:.2f}%")

cm = confusion_matrix(y_test, y_pred)
print(f"\nğŸ“Š Confusion Matrix:")
print(f"   TN: {cm[0][0]}, FP: {cm[0][1]}")
print(f"   FN: {cm[1][0]}, TP: {cm[1][1]}")

# Feature Importance
print(f"\nğŸ” En Ã–nemli 10 Ã–zellik:")
feature_importance = pd.DataFrame({
    'feature': feature_cols,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

for idx, row in feature_importance.head(10).iterrows():
    print(f"   {row['feature']}: {row['importance']:.4f}")

# ============================================================================
# 10. SHAP EXPLAINER
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ” SHAP EXPLAINER")
print("=" * 80)

print(f"\nğŸš€ SHAP hesaplanÄ±yor...")
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_train_scaled[:1000])
print("âœ… SHAP explainer oluÅŸturuldu")

# ============================================================================
# 11. MODEL KAYDET VE Ä°NDÄ°R
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ’¾ MODEL DOSYALARI")
print("=" * 80)

# DosyalarÄ± kaydet
with open('churn_model.pkl', 'wb') as f:
    pickle.dump(model, f)
with open('scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)
with open('label_encoders.pkl', 'wb') as f:
    pickle.dump(label_encoders, f)
with open('feature_names.pkl', 'wb') as f:
    pickle.dump(feature_cols, f)

metrics = {
    'accuracy': float(accuracy),
    'precision': float(precision),
    'recall': float(recall),
    'f1_score': float(f1),
    'roc_auc': float(roc_auc),
    'confusion_matrix': cm.tolist(),
    'feature_importance': feature_importance.to_dict('records')
}

with open('model_metrics.pkl', 'wb') as f:
    pickle.dump(metrics, f)

with open('churn_model.json', 'w') as f:
    json.dump({
        'model_type': 'XGBoost Hybrid',
        'datasets': 'Maven Analytics + Iranian Churn',
        'n_samples': len(df_combined),
        'maven_samples': len(maven_standard),
        'iranian_samples': len(iranian_standard),
        'n_features': len(feature_cols),
        'metrics': {
            'accuracy': f"{accuracy*100:.2f}%",
            'precision': f"{precision*100:.2f}%",
            'recall': f"{recall*100:.2f}%",
            'f1_score': f"{f1*100:.2f}%",
            'roc_auc': f"{roc_auc*100:.2f}%"
        },
        'features': feature_cols
    }, f, indent=2)

print("âœ… Dosyalar kaydedildi")

# Zip ve indir
import zipfile
with zipfile.ZipFile('aura_hybrid_models.zip', 'w') as zipf:
    zipf.write('churn_model.pkl')
    zipf.write('scaler.pkl')
    zipf.write('label_encoders.pkl')
    zipf.write('feature_names.pkl')
    zipf.write('model_metrics.pkl')
    zipf.write('churn_model.json')

files.download('aura_hybrid_models.zip')

print("\n" + "=" * 80)
print("ğŸ‰ HYBRID MODEL TAMAMLANDI!")
print("=" * 80)
print(f"\nâœ… Ä°ki dataset baÅŸarÄ±yla birleÅŸtirildi ve model eÄŸitildi!")
print(f"\nğŸ“Š Model Ã–zeti:")
print(f"   Toplam mÃ¼ÅŸteri: {len(df_combined)}")
print(f"   Maven: {len(maven_standard)}")
print(f"   Iranian: {len(iranian_standard)}")
print(f"   Ã–zellik sayÄ±sÄ±: {len(feature_cols)}")
print(f"   Accuracy: {accuracy*100:.2f}%")
print(f"   ROC AUC: {roc_auc*100:.2f}%")
print(f"\nğŸš€ aura_hybrid_models.zip indirildi!")
print(f"   DosyalarÄ± aura-backend/models/ klasÃ¶rÃ¼ne Ã§Ä±kart")
