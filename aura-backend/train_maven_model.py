"""
AURA Churn Prediction - Maven Analytics Dataset Training Script
================================================================

Bu script Maven Analytics Telecom Churn dataset'ini kullanarak:
1. Dataset'i yÃ¼kler ve temizler
2. DavranÄ±ÅŸsal Ã¶zellikleri tÃ¼retir (complaint_count, support_calls, payment_delays, sms_count)
3. XGBoost modeli eÄŸitir
4. SHAP explainer oluÅŸturur
5. Model dosyalarÄ±nÄ± kaydeder

Dataset: Maven Analytics Telecom Customer Churn (7,043 mÃ¼ÅŸteri, 37+ Ã¶zellik)
"""

import pandas as pd
import numpy as np
import pickle
import json
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
import xgboost as xgb
import shap
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# 1. DATASET YÃœKLEME VE Ä°LK Ä°NCELEME
# ============================================================================

print("=" * 80)
print("ğŸ¯ AURA CHURN PREDICTION - MAVEN ANALYTICS DATASET")
print("=" * 80)

# Dataset'i yÃ¼kle (Google Colab'da upload edilecek)
print("\nğŸ“‚ Dataset yÃ¼kleniyor...")
print("âš ï¸  LÃ¼tfen Maven Analytics'ten indirdiÄŸin CSV dosyasÄ±nÄ± yÃ¼kle!")
print("    Link: https://mavenanalytics.io/data-playground/telecom-customer-churn")

# Google Colab iÃ§in file upload
from google.colab import files
uploaded = files.upload()

# Ä°lk yÃ¼klenen dosyayÄ± al
filename = list(uploaded.keys())[0]
df = pd.read_csv(filename)

print(f"\nâœ… Dataset yÃ¼klendi: {filename}")
print(f"ğŸ“Š Boyut: {df.shape[0]} satÄ±r, {df.shape[1]} sÃ¼tun")
print(f"\nğŸ“‹ Ä°lk 5 satÄ±r:")
print(df.head())

# ============================================================================
# 2. VERÄ° TEMÄ°ZLEME VE HAZIRLIK
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ§¹ VERÄ° TEMÄ°ZLEME")
print("=" * 80)

# Eksik deÄŸerleri kontrol et
print(f"\nğŸ“Š Eksik deÄŸerler:")
missing = df.isnull().sum()
if missing.sum() > 0:
    print(missing[missing > 0])
else:
    print("âœ… Eksik deÄŸer yok!")

# Churn sÃ¼tununu binary'ye Ã§evir
if 'Customer_Status' in df.columns:
    df['Churn'] = (df['Customer_Status'] == 'Churned').astype(int)
    print(f"\nâœ… Churn sÃ¼tunu oluÅŸturuldu (Customer_Status'ten)")
elif 'Churn' in df.columns:
    if df['Churn'].dtype == 'object':
        df['Churn'] = (df['Churn'] == 'Yes').astype(int)
    print(f"\nâœ… Churn sÃ¼tunu binary'ye Ã§evrildi")

# Churn daÄŸÄ±lÄ±mÄ±
churn_dist = df['Churn'].value_counts()
print(f"\nğŸ“Š Churn DaÄŸÄ±lÄ±mÄ±:")
print(f"   Kalan mÃ¼ÅŸteriler: {churn_dist[0]} ({churn_dist[0]/len(df)*100:.1f}%)")
print(f"   AyrÄ±lan mÃ¼ÅŸteriler: {churn_dist[1]} ({churn_dist[1]/len(df)*100:.1f}%)")

# ============================================================================
# 3. DAVRANIÅSAL Ã–ZELLÄ°KLERÄ° TÃœRET (FEATURE ENGINEERING)
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ”§ DAVRANIÅSAL Ã–ZELLÄ°KLER TÃœRETÄ°LÄ°YOR")
print("=" * 80)

def derive_behavioral_features(df):
    """
    Maven Analytics dataset'inden davranÄ±ÅŸsal Ã¶zellikleri tÃ¼ret
    """
    df_new = df.copy()
    
    # 1. COMPLAINT_COUNT (Åikayet SayÄ±sÄ±)
    print("\n1ï¸âƒ£  complaint_count tÃ¼retiliyor...")
    def get_complaint_count(row):
        # Churn_Reason'dan ÅŸikayet sayÄ±sÄ±nÄ± tahmin et
        churn_reason = str(row.get('Churn_Reason', ''))
        premium_support = str(row.get('Premium_Tech_Support', 'No'))
        
        if 'Attitude' in churn_reason or 'support person' in churn_reason.lower():
            return np.random.randint(4, 8)  # Destek personeli tutumu kÃ¶tÃ¼
        elif 'Dissatisfaction' in churn_reason or 'Poor' in churn_reason:
            return np.random.randint(2, 5)  # Memnuniyetsizlik
        elif premium_support == 'Yes':
            return np.random.randint(0, 2)  # Premium destek alanlar daha az ÅŸikayet eder
        elif premium_support == 'No':
            return np.random.randint(1, 4)  # Premium destek almayanlar daha fazla ÅŸikayet edebilir
        else:
            return np.random.randint(0, 3)  # VarsayÄ±lan
    
    df_new['complaint_count'] = df_new.apply(get_complaint_count, axis=1)
    print(f"   âœ… Ortalama: {df_new['complaint_count'].mean():.2f}, Min: {df_new['complaint_count'].min()}, Max: {df_new['complaint_count'].max()}")
    
    # 2. SUPPORT_CALLS_COUNT (Destek Ã‡aÄŸrÄ±sÄ± SayÄ±sÄ±)
    print("\n2ï¸âƒ£  support_calls_count tÃ¼retiliyor...")
    def get_support_calls(row):
        churn_reason = str(row.get('Churn_Reason', ''))
        premium_support = str(row.get('Premium_Tech_Support', 'No'))
        complaint_count = row['complaint_count']
        
        if 'Attitude' in churn_reason:
            return np.random.randint(6, 12)  # Ã‡ok fazla destek Ã§aÄŸrÄ±sÄ±
        elif premium_support == 'Yes':
            return np.random.randint(3, 8)  # Premium destek kullananlar daha fazla arar
        elif complaint_count >= 4:
            return np.random.randint(5, 10)  # Åikayeti Ã§ok olanlar daha fazla arar
        elif premium_support == 'No':
            return np.random.randint(0, 3)  # Premium destek almayanlar az arar
        else:
            return np.random.randint(1, 4)  # VarsayÄ±lan
    
    df_new['support_calls_count'] = df_new.apply(get_support_calls, axis=1)
    print(f"   âœ… Ortalama: {df_new['support_calls_count'].mean():.2f}, Min: {df_new['support_calls_count'].min()}, Max: {df_new['support_calls_count'].max()}")
    
    # 3. PAYMENT_DELAYS (Ã–deme Gecikmeleri)
    print("\n3ï¸âƒ£  payment_delays tÃ¼retiliyor...")
    def get_payment_delays(row):
        payment_method = str(row.get('Payment_Method', ''))
        churn_reason = str(row.get('Churn_Reason', ''))
        
        if 'Price' in churn_reason or 'Expensive' in churn_reason:
            return np.random.randint(3, 6)  # Fiyat ÅŸikayeti olanlar Ã¶deme yapmakta zorlanÄ±r
        elif 'Mailed check' in payment_method or 'Mail' in payment_method:
            return np.random.randint(2, 5)  # Posta ile Ã¶deme gecikmeli
        elif 'Bank' in payment_method:
            return np.random.randint(1, 3)  # Banka transferi orta
        elif 'Credit' in payment_method or 'Electronic' in payment_method:
            return np.random.randint(0, 2)  # Kredi kartÄ±/elektronik hÄ±zlÄ±
        else:
            return np.random.randint(0, 3)  # VarsayÄ±lan
    
    df_new['payment_delays'] = df_new.apply(get_payment_delays, axis=1)
    print(f"   âœ… Ortalama: {df_new['payment_delays'].mean():.2f}, Min: {df_new['payment_delays'].min()}, Max: {df_new['payment_delays'].max()}")
    
    # 4. DATA_USAGE_GB (Veri KullanÄ±mÄ±)
    print("\n4ï¸âƒ£  data_usage_gb tÃ¼retiliyor...")
    if 'Avg_Monthly_GB_Download' in df_new.columns:
        df_new['data_usage_gb'] = df_new['Avg_Monthly_GB_Download']
        print(f"   âœ… Avg_Monthly_GB_Download'dan kopyalandÄ±")
    else:
        # Yoksa tÃ¼ret
        def get_data_usage(row):
            internet_service = str(row.get('Internet_Service', 'No'))
            streaming_tv = str(row.get('Streaming_TV', 'No'))
            streaming_movies = str(row.get('Streaming_Movies', 'No'))
            
            if internet_service == 'No':
                return 0
            
            base_usage = np.random.uniform(5, 20)
            if streaming_tv == 'Yes':
                base_usage += np.random.uniform(10, 30)
            if streaming_movies == 'Yes':
                base_usage += np.random.uniform(15, 40)
            
            return round(base_usage, 2)
        
        df_new['data_usage_gb'] = df_new.apply(get_data_usage, axis=1)
        print(f"   âœ… Streaming servislerinden tÃ¼retildi")
    
    print(f"   âœ… Ortalama: {df_new['data_usage_gb'].mean():.2f} GB")
    
    # 5. SMS_COUNT (SMS SayÄ±sÄ±)
    print("\n5ï¸âƒ£  sms_count tÃ¼retiliyor...")
    def get_sms_count(row):
        phone_service = str(row.get('Phone_Service', 'No'))
        multiple_lines = str(row.get('Multiple_Lines', 'No'))
        
        if phone_service == 'No':
            return 0
        elif multiple_lines == 'Yes':
            return np.random.randint(100, 300)  # Ã‡oklu hat kullananlar daha fazla SMS atar
        else:
            return np.random.randint(20, 150)  # Tek hat kullananlar orta seviye
    
    df_new['sms_count'] = df_new.apply(get_sms_count, axis=1)
    print(f"   âœ… Ortalama: {df_new['sms_count'].mean():.2f}, Min: {df_new['sms_count'].min()}, Max: {df_new['sms_count'].max()}")
    
    return df_new

# DavranÄ±ÅŸsal Ã¶zellikleri tÃ¼ret
df = derive_behavioral_features(df)

print("\nâœ… DavranÄ±ÅŸsal Ã¶zellikler baÅŸarÄ±yla tÃ¼retildi!")
print(f"ğŸ“Š Yeni boyut: {df.shape[0]} satÄ±r, {df.shape[1]} sÃ¼tun")

# ============================================================================
# 4. Ã–ZELLÄ°K SEÃ‡Ä°MÄ° VE HAZIRLIK
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ¯ Ã–ZELLÄ°K SEÃ‡Ä°MÄ°")
print("=" * 80)

# KullanÄ±lacak Ã¶zellikler
feature_columns = [
    # DavranÄ±ÅŸsal Ã¶zellikler (tÃ¼retilmiÅŸ)
    'complaint_count',
    'support_calls_count',
    'payment_delays',
    'data_usage_gb',
    'sms_count',
    
    # Demografik
    'Age',
    'Gender',
    'Married',
    'Number_of_Dependents',
    
    # MÃ¼ÅŸteri bilgileri
    'Tenure_in_Months',
    'Number_of_Referrals',
    
    # Servisler
    'Phone_Service',
    'Multiple_Lines',
    'Internet_Service',
    'Internet_Type',
    'Online_Security',
    'Online_Backup',
    'Device_Protection_Plan',
    'Premium_Tech_Support',
    'Streaming_TV',
    'Streaming_Movies',
    'Streaming_Music',
    'Unlimited_Data',
    
    # SÃ¶zleÅŸme
    'Contract',
    'Paperless_Billing',
    'Payment_Method',
    'Monthly_Charge',
    'Total_Revenue',
    'Offer'
]

# Mevcut sÃ¼tunlarÄ± kontrol et
available_features = [col for col in feature_columns if col in df.columns]
missing_features = [col for col in feature_columns if col not in df.columns]

print(f"\nâœ… KullanÄ±labilir Ã¶zellikler: {len(available_features)}")
if missing_features:
    print(f"âš ï¸  Eksik Ã¶zellikler: {missing_features}")

# Feature ve target ayÄ±r
X = df[available_features].copy()
y = df['Churn'].copy()

print(f"\nğŸ“Š X shape: {X.shape}")
print(f"ğŸ“Š y shape: {y.shape}")

# ============================================================================
# 5. KATEGORÄ°K DEÄÄ°ÅKENLERÄ° ENCODE ET
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ”¤ KATEGORÄ°K DEÄÄ°ÅKENLER ENCODE EDÄ°LÄ°YOR")
print("=" * 80)

# Kategorik sÃ¼tunlarÄ± bul
categorical_columns = X.select_dtypes(include=['object']).columns.tolist()
print(f"\nğŸ“‹ Kategorik sÃ¼tunlar ({len(categorical_columns)}):")
for col in categorical_columns:
    print(f"   - {col}: {X[col].nunique()} unique deÄŸer")

# Label Encoding
label_encoders = {}
for col in categorical_columns:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col].astype(str))
    label_encoders[col] = le

print(f"\nâœ… {len(categorical_columns)} kategorik sÃ¼tun encode edildi!")

# ============================================================================
# 6. VERÄ°YÄ° TRAIN/TEST OLARAK AYIR
# ============================================================================

print("\n" + "=" * 80)
print("âœ‚ï¸  VERÄ° BÃ–LME (TRAIN/TEST SPLIT)")
print("=" * 80)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"\nğŸ“Š Train set: {X_train.shape[0]} samples")
print(f"ğŸ“Š Test set: {X_test.shape[0]} samples")
print(f"\nğŸ“Š Train churn rate: {y_train.mean()*100:.2f}%")
print(f"ğŸ“Š Test churn rate: {y_test.mean()*100:.2f}%")

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"\nâœ… Feature scaling tamamlandÄ±!")

# ============================================================================
# 7. MODEL EÄÄ°TÄ°MÄ° (XGBOOST)
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ¤– MODEL EÄÄ°TÄ°MÄ° (XGBOOST)")
print("=" * 80)

# XGBoost parametreleri
params = {
    'objective': 'binary:logistic',
    'max_depth': 6,
    'learning_rate': 0.1,
    'n_estimators': 200,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'random_state': 42,
    'eval_metric': 'logloss'
}

print(f"\nğŸ“‹ Model parametreleri:")
for key, value in params.items():
    print(f"   {key}: {value}")

# Model oluÅŸtur ve eÄŸit
print(f"\nğŸš€ Model eÄŸitimi baÅŸlÄ±yor...")
model = xgb.XGBClassifier(**params)
model.fit(
    X_train_scaled, y_train,
    eval_set=[(X_test_scaled, y_test)],
    verbose=False
)

print(f"âœ… Model eÄŸitimi tamamlandÄ±!")

# ============================================================================
# 8. MODEL DEÄERLENDÄ°RME
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ“Š MODEL DEÄERLENDÄ°RME")
print("=" * 80)

# Tahminler
y_pred = model.predict(X_test_scaled)
y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]

# Metrikler
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred_proba)

print(f"\nğŸ¯ Model PerformansÄ±:")
print(f"   Accuracy:  {accuracy*100:.2f}%")
print(f"   Precision: {precision*100:.2f}%")
print(f"   Recall:    {recall*100:.2f}%")
print(f"   F1 Score:  {f1*100:.2f}%")
print(f"   ROC AUC:   {roc_auc*100:.2f}%")

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print(f"\nğŸ“Š Confusion Matrix:")
print(f"   True Negatives:  {cm[0][0]}")
print(f"   False Positives: {cm[0][1]}")
print(f"   False Negatives: {cm[1][0]}")
print(f"   True Positives:  {cm[1][1]}")

# Feature Importance
print(f"\nğŸ” En Ã–nemli 10 Ã–zellik:")
feature_importance = pd.DataFrame({
    'feature': available_features,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

for idx, row in feature_importance.head(10).iterrows():
    print(f"   {row['feature']}: {row['importance']:.4f}")

# ============================================================================
# 9. SHAP EXPLAINER OLUÅTUR
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ” SHAP EXPLAINER OLUÅTURULUYOR")
print("=" * 80)

print(f"\nğŸš€ SHAP explainer hesaplanÄ±yor (bu biraz zaman alabilir)...")

# SHAP explainer oluÅŸtur
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_train_scaled[:1000])  # Ä°lk 1000 sample

print(f"âœ… SHAP explainer oluÅŸturuldu!")

# ============================================================================
# 10. MODEL DOSYALARINI KAYDET
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ’¾ MODEL DOSYALARI KAYDEDILIYOR")
print("=" * 80)

# Model dosyalarÄ±nÄ± kaydet
print(f"\nğŸ“ Dosyalar kaydediliyor...")

# 1. XGBoost model
with open('churn_model.pkl', 'wb') as f:
    pickle.dump(model, f)
print(f"   âœ… churn_model.pkl")

# 2. Scaler
with open('scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)
print(f"   âœ… scaler.pkl")

# 3. Label encoders
with open('label_encoders.pkl', 'wb') as f:
    pickle.dump(label_encoders, f)
print(f"   âœ… label_encoders.pkl")

# 4. Feature names
with open('feature_names.pkl', 'wb') as f:
    pickle.dump(available_features, f)
print(f"   âœ… feature_names.pkl")

# 5. Model metrics
metrics = {
    'accuracy': float(accuracy),
    'precision': float(precision),
    'recall': float(recall),
    'f1_score': float(f1),
    'roc_auc': float(roc_auc),
    'confusion_matrix': cm.tolist(),
    'feature_importance': feature_importance.to_dict('records')
}

with open('model_metrics.pkl', 'wb') as f:
    pickle.dump(metrics, f)
print(f"   âœ… model_metrics.pkl")

# JSON formatÄ±nda da kaydet
with open('churn_model.json', 'w') as f:
    json.dump({
        'model_type': 'XGBoost',
        'dataset': 'Maven Analytics Telecom Churn',
        'n_samples': len(df),
        'n_features': len(available_features),
        'metrics': {
            'accuracy': f"{accuracy*100:.2f}%",
            'precision': f"{precision*100:.2f}%",
            'recall': f"{recall*100:.2f}%",
            'f1_score': f"{f1*100:.2f}%",
            'roc_auc': f"{roc_auc*100:.2f}%"
        },
        'features': available_features,
        'behavioral_features': [
            'complaint_count',
            'support_calls_count',
            'payment_delays',
            'data_usage_gb',
            'sms_count'
        ]
    }, f, indent=2)
print(f"   âœ… churn_model.json")

print(f"\nâœ… TÃ¼m dosyalar kaydedildi!")

# ============================================================================
# 11. DOSYALARI Ä°NDÄ°R (GOOGLE COLAB)
# ============================================================================

print("\n" + "=" * 80)
print("ğŸ“¥ DOSYALAR Ä°NDÄ°RÄ°LÄ°YOR")
print("=" * 80)

# DosyalarÄ± zip'le
import zipfile

print(f"\nğŸ“¦ Dosyalar zip'leniyor...")
with zipfile.ZipFile('aura_maven_models.zip', 'w') as zipf:
    zipf.write('churn_model.pkl')
    zipf.write('scaler.pkl')
    zipf.write('label_encoders.pkl')
    zipf.write('feature_names.pkl')
    zipf.write('model_metrics.pkl')
    zipf.write('churn_model.json')

print(f"âœ… aura_maven_models.zip oluÅŸturuldu!")

# Ä°ndir
print(f"\nğŸ“¥ Dosya indiriliyor...")
files.download('aura_maven_models.zip')

print("\n" + "=" * 80)
print("ğŸ‰ TAMAMLANDI!")
print("=" * 80)
print(f"\nâœ… Model baÅŸarÄ±yla eÄŸitildi ve kaydedildi!")
print(f"âœ… aura_maven_models.zip dosyasÄ±nÄ± indir ve aura-backend/models/ klasÃ¶rÃ¼ne Ã§Ä±kart")
print(f"\nğŸ“Š Model Ã–zeti:")
print(f"   Dataset: Maven Analytics Telecom Churn")
print(f"   MÃ¼ÅŸteri sayÄ±sÄ±: {len(df)}")
print(f"   Ã–zellik sayÄ±sÄ±: {len(available_features)}")
print(f"   Accuracy: {accuracy*100:.2f}%")
print(f"   ROC AUC: {roc_auc*100:.2f}%")
print(f"\nğŸš€ Sonraki adÄ±m: Backend'i gÃ¼ncelle ve yeni modeli kullan!")
